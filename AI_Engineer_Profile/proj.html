<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Jahnvi Paliwal | Projects</title>

    <!-- Main global styles (for sidebar, colors, fonts) -->
    <!-- <link rel="stylesheet" href="style.css"> -->

    <!-- Project specific styles -->
    <link rel="stylesheet" href="style_proj.css">
</head>
<body>

<!-- SIDEBAR (UNCHANGED, reused) -->
<nav class="sidebar">
    <ul>
        <li><a href="AI_eng_home.html">Home</a></li>
            <li><a href="exp.html">Experience</a></li>
            <li><a href="certi.html">Certifications</a></li>
            <li><a href="proj.html">Projects</a></li>
            <li class="social-icons">
                <a href="mailto:paliwaljnv08@gmail.com" aria-label="Email">
                    <img src="../icons/email.svg" alt="Email">
                </a>
                <a href="https://www.linkedin.com/in/jahnvipaliwal/" target="_blank" aria-label="LinkedIn">
                    <img src="../icons/linkedin.svg" alt="LinkedIn">
                </a>
                <a href="https://github.com/jahnvipaliwal" target="_blank" aria-label="GitHub">
                    <img src="../icons/github.svg" alt="GitHub">
                </a>
                
                <a href="https://www.hackerrank.com/profile/paliwaljnv08" target="_blank" aria-label="HackerRank">
                    <img src="../icons/hackerrank.svg" alt="HackerRank">
                </a>
                <a href="https://www.kaggle.com/jahnvipaliwal" target="_blank" aria-label="Kaggle">
                    <img src="../icons/kaggle.svg" alt="Kaggle">
                </a>
                <a href="https://leetcode.com/u/RoE440RLSy/" target="_blank" aria-label="LeetCode">
                    <img src="../icons/leetcode.svg" alt="LeetCode">
                </a>
            </li>
            <li><a href="../index.html">Profile<br>Switch</a></li>
    </ul>
</nav>

<!-- PROJECTS MAIN CONTAINER -->
<main class="main-content projects-container">

    <!-- PROJECT 1 -->
    <!-- <section class="project-box">
        <div class="project-left">
            <h2>NewsRoom</h2>

            <p>
                <h3>Problem Statement</h3>
                Room
            </p>
            <p>
                <h3>Solution Overview</h3>
                An 
            </p>
            <p>
                <h3>Progress so far . . .</h3>
                <ul>
                    <li>OpenAI API integrated for RAG-based insights</li>
                    <li>ResNet model training in progress (testing other algorithms)</li>
                    <li>Mathematical and statistical functions implemented</li>
                    <li>Creating custom dataset due to lack of public data</li>
                    <li>289 prediction samples created till now</li>
                </ul>
            </p>

            <!-- Tech Stack -->
            <!-- <div class="tech-stack">
                <span>OpenAI GPT-3</span>
                <span>RAG</span>
                <span>TF-IDF</span>
                <span>Logistic Regression</span>
                <span>ARIMA</span>
                <span>Streamlit</span>
                <span>Librosa</span>
                <span>NLP</span>
                <span>Isolation Forest</span>
                <span>Flask</span>
                <span>TensorFlow</span>
                <span>Pandas</span>
                <span>RAG</span>
                <span>OpenAI</span>
            </div>
        </div>

        <div class="project-right">
            <img src="../images/AI_logo.png" alt="Project Screenshot">

            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/Product-Performance-Retrieval-Agent" target="_blank">GitHub</a> -->
                <!-- <a href="#" target="_blank">Live Demo</a>
                <a href="#" target="_blank">Paper</a> -->
            <!-- </div>
        </div>
    </section> --> 
    
    <!-- PROJECT 1 -->
    <section class="project-box">
        <div class="project-left">
            <h2>Product Performance Retrieval Agent</h2>

            <p>
                <h3>Problem Statement</h3>
                Raw financial CSV datasets are difficult to interpret, 
                analyze, and forecast without technical expertise, 
                limiting data-driven decision-making.
            </p>
            <p>
                <h3>Solution Overview</h3>
                An AI-powered agent that automatically profiles Excel/CSV datasets,suggests insights, performs analysis, 
                visualizes results, and provides a data quality score with interactive Q&A.
            </p>
            <p>
                <h3>How I Solved the Problem</h3>
                I designed a modular pipeline that begins with a dataset profiler extracting metadata like column types, 
                missing values, cardinality, and distributions. This metadata is sent to an LLM reasoning layer 
                (Groq LLaMA 3.1), which dynamically suggests six relevant insights tailored to the dataset. 
                An insight router maps LLM recommendations to pre-defined deterministic Python functions that 
                perform analysis such as outlier detection, correlation analysis, distribution inspection, and missing 
                value evaluation. The results are visualized automatically using Streamlit charts, and a data quality 
                score quantifies the reliability of the dataset. The system also supports follow-up Q&A, allowing users 
                to query results interactively. Users enter their own API key, ensuring secure and private LLM integration. 
                The pipeline is optimized to prevent stale results when switching datasets and maintains efficient token usage.
            </p>

            <!-- Tech Stack -->
            <div class="tech-stack">
                <span>OpenAI GPT-3</span>
                <span>RAG</span>
                <span>Python</span>
                <span>Streamlit</span>
                <span>Pandas</span>
                <span>NumPy</span>
                <span>Groq API</span>
                <span>LLaMA 3.1</span>
                <span>Deterministic analysis functions Forest</span>
                <span>Automatic charting</span>
                <span>TensorFlow</span>
                <span>Session-aware state management</span>
                <span>Follow-up interactive Q&A</span>
                <span>CSV and Excel file handling</span>
            </div>
        </div>

        <div class="project-right">
            <img src="../images/AI_logo.png" alt="Project Screenshot">

            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/Product-Performance-Retrieval-Agent" target="_blank">GitHub</a>
                <a href="https://jd-data-retrieval-agent.streamlit.app/" target="_blank">Live Demo</a>
                <a href="https://drive.google.com/file/d/15SawWTbBQvPor4UpLHn8isWMVbgibpEI/view?usp=sharing" target="_blank">Detailed Report</a>
            </div>
        </div>
    </section>
   
    <!-- PROJECT 3 -->

    <section class="project-box">
        <div class="project-left">
            <h2>Intelligent Brawl Monitor</h2>

            <p>
                <h3>Problem Statement</h3>
                Violence and aggressive behavior often escalate without early warning, making it difficult to ensure timely intervention for public safety, workplace security, and women’s safety.
            </p>
            <p>
                <h3>Solution Overview</h3>
                Built a real-time AI-based surveillance system that detects human aggression through facial expressions and body posture and sends automated SOS alerts for immediate response.
            </p>
            <p>
                <h3>How I Solved the Problem</h3>
                I architected a real-time video analytics pipeline that captures live camera feeds and processes each frame with low latency using OpenCV. Facial regions are detected and analyzed using machine learning models trained on public emotion datasets to identify anger-related expressions, while pose estimation frameworks extract human keypoints to classify aggressive body stances. I implemented rule-based fusion logic to ensure alerts are triggered only when both emotional and postural aggression signals are present, reducing false positives. The system integrates automated email notifications for real-time escalation and is designed to be extensible for IoT devices, mobile alerts, and future deep learning upgrades. The solution emphasizes scalability, ethical deployment, and real-world reliability under varying environmental conditions.
            </p>

            <div class="tech-stack">
                <span>Python</span>
                <span>real-time pipelines</span>
                <span>OpenCV</span>
                <span>image processing</span>
                <span>face detection</span>
                <span>keypoint-based pose analysis</span>
                <span>CNN-based emotion recognition</span>
                <span>supervised learning</span>
                <span>model inferences</span>
                <span>transfer learning concept</span>
                <span>MediaPipe</span>
                <span>OpenPose</span>
                <span>email alert systems</span>
                <span>Data & Model Handling</span>
                <span>event-driven trigger</span>
                <span>camera-based systems</span>
                <span>privacy-aware AI design</span>

            </div>
        </div>

        <div class="project-right">
            <img src="../images/I_Brawl_Monitor_Logo.png" alt="Project Screenshot">

            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/Intelligent_Brawl_monitor">View code on GitHub</a>
                <!-- <a href="#">Working Sample</a>
                <a href="#">More details</a> -->
            </div>
        </div>
    </section>


    
     <section class="project-box">
        <div class="project-left">
            <h2>Cross-Camera Human Identification System</h2>
        
            <!-- Problem Statement -->
            <section class="problem-statement">
                <h3><strong>Problem Statement</strong></h3>
                <p>
                    Multi-camera human tracking in dynamic environments is challenging due to varying viewpoints, 
                    occlusions, and lighting conditions. Existing systems often fail to maintain consistent identities 
                    across multiple cameras without relying on facial recognition.
                </p>
            </section>
        
            <!-- Solution Overview -->
            <section class="solution-overview">
                <h3><strong>Solution Overview</strong></h3>
                <p>
                    Developed a view-invariant person re-identification pipeline that combines YOLOv8 detection, 
                    BoT-SORT multi-object tracking, color histogram embeddings, pose-aware spatial features, and 
                    temporal trajectory encoding. The system matches identities across camera views and outputs videos 
                    with consistent global IDs while preserving the original footage.
                </p>
            </section>
        
            <!-- How I Solved the Problem -->
            <section class="solution-details">
                <h3><strong>How I Solved the Problem</strong></h3>
                <p>
                    The pipeline starts with YOLOv8 detecting human players in each video feed. BoT-SORT assigns 
                    temporary track IDs within each camera. For cross-camera association, I extracted color histogram 
                    embeddings for each track and incorporated motion-consistency via temporal trajectory comparison. 
                    A matching algorithm based on Bhattacharyya distance and cosine similarity aligns tracks across 
                    camera views, producing a global ID map. The pipeline outputs separate MP4 files for each camera 
                    with bounding boxes and global IDs overlaid, leaving the original videos unchanged. Evaluation 
                    metrics include average cosine similarity, coverage of matched tracks, track length statistics, 
                    and motion stability.
                </p>
            </section>
        
            <!-- Tech Stack -->
            <section class="tech-stack-section">
                <div class="tech-stack">
                    <span>Python</span>
                    <span>OpenCV</span>
                    <span>Ultralytics YOLOv8</span>
                    <span>BoT-SORT</span>
                    <span>NVIDIA T4 GPU</span>
                    <span>NumPy</span>
                    <span>Streamlit (optional for visualization)</span>
                    <span>Bhattacharyya distance & cosine similarity</span>
                    <span>Temporal trajectory encoding</span>
                    <span>Video I/O (MP4 output)</span>
                </div>
            </section>
        </div>

    <!-- Optional Images / Diagrams -->
        <div class="project-right">
            <img src="../images/struct.png" alt="Project Screenshot">

            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/Cross-Camera-Human-Identification-System">View code on GitHub</a>
                <!-- <!-- <a href="#">Working Sample</a> -->
                <a href="https://drive.google.com/file/d/1xKj6DjLxfcQk4eSrdi1lPut8xl6K-BMs/view?usp=sharing">View Report</a> 
            </div>
        </div>
     </section>    

    <!-- PROJECT 2 -->
    <section class="project-box">
        <div class="project-left">
            <h2>Cognitive Stress Analysis</h2>
            <h3>Problem Statement</h3>    
            <p>
                Cognitive stress influences speech patterns but is difficult to detect objectively without intrusive methods or labeled data. The challenge was to build a non-invasive, automated system that identifies stress indicators from raw speech using data-driven techniques.
            </p>
            <p>
                <h3>Solution Overview</h3>
                Built an end-to-end speech analysis pipeline combining acoustic features, NLP, and unsupervised machine learning to quantify and compare cognitive stress indicators.
            </p>
            <p>
                <h3>How I Solved the Problem</h3>
                I implemented a modular Python-based pipeline that performs speech recognition, acoustic signal processing, and linguistic feature engineering to capture speech rate, pitch statistics, pause patterns, hesitation markers, and sentence complexity. These features are normalized and analyzed using K-Means clustering, Isolation Forest anomaly detection, and cosine similarity to identify deviations and stress-related patterns. The system automatically generates CSV reports containing feature metrics and model outputs, ensuring scalability, interpretability, and extensibility.
            </p>

            <div class="tech-stack">
                <span>Python</span>
                <span>audio signal processing</span>
                <span>Isolation Forest</span>
                <span>NLP</span>
                <span>feature engineering</span>
                <span>unsupervised learning</span>
                <span>anomaly detection</span>
                <span>librosa</span>
                <span>SpeechRecognition</span>
                <span>nltk</span>
                <span>scikit-learn</span>
                <span>pandas</span>
                <span>numpy</span>
                <span>data pipelines</span>
                <span>Ethical AI practices</span>

            </div>
        </div>

        <div class="project-right">
            <img src="../images/cognitive_stress_logo.png" alt="Project Screenshot">

            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/Cognitive-Stress-Analysis">View code on GitHub</a>
                <a href="https://jd-cognitive-stress-analysis.streamlit.app/">Live App</a>
                <!-- <a href="#">More details</a> -->
            </div>
        </div>
    </section>

    <!-- PROJECT 3 -->

    <section class="project-box"> 
        <div class="project-left">
            <h2>Open-Vocabulary Object Detection Study</h2>
    
            <p>
                <h3>Problem Statement</h3>
                Traditional object detection models are limited to predefined categories, 
                making it difficult to detect unseen or custom objects dynamically. 
                Evaluating speed, accuracy, and generalization across models for real-time 
                Video analysis is also challenging.
            </p>
            <p>
                <h3>Solution Overview</h3>
                Implemented a real-time, zero-shot object detection pipeline using OWL-ViT, 
                enabling dynamic text-based queries without reliance on fixed training datasets. 
                YOLOv8 was used as a benchmark for comparison. The system processes video frames, 
                draws bounding boxes, logs per-class detections and confidence scores, and allows 
                side-by-side visual and statistical analysis of both models.
            </p>
            <p>
                <h3>How I Solved the Problem</h3>
                I developed a modular pipeline that captures video frames from Google Drive, 
                resizes them for faster processing, and performs object detection using both 
                YOLOv8 and OWL-ViT. Each frame’s predictions are annotated with bounding boxes 
                and confidence scores. A CSV log records per-frame, per-class detections for 
                statistical analysis. Using Python libraries like pandas, matplotlib, and seaborn, 
                I generated detection counts, confidence distribution plots, false positive analysis, 
                and T-test statistical comparison. Output videos show side-by-side model predictions 
                for qualitative evaluation. The pipeline is optimized with GPU acceleration, mixed 
                precision, and frame subsampling to balance speed and accuracy.
            </p>
    
            <!-- Tech Stack -->
            <div class="tech-stack">
                <span>Python</span>
                <span>PyTorch</span>
                <span>Transformers</span>
                <span>OpenCV</span>
                <span>Pillow</span>
                <span>Matplotlib</span>
                <span>Seaborn</span>
                <span>Hugging Face OWL-ViT</span>
                <span>YOLOv8 (Ultralytics)</span>
                <span>Google Colab</span>
                <span>GPU Acceleration</span>
                <span>Zero-Shot Object Detection</span>
                <span>CSV Logging</span>
                <span>Video Annotation & Frame Processing</span>
            </div>
        </div>
    
        <div class="project-right">
            <img src="../images/detection_count_barplot.png" alt="Project Screenshot">
    
            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/Open-Vocabulary-Object-Detection-Study" target="_blank">View code on GitHub</a>
                <!-- <a href="https://colab.research.google.com/drive/your_colab_notebook_link" target="_blank">Live Demo</a> -->
                <a href="https://drive.google.com/file/d/1XyTPMOjWSePmSW8oHYamA9q1GqU87zFQ/view?usp=sharing" target="_blank">View Report</a>
            </div>
        </div>
    </section>

     <!-- PROJECT 4 -->
    
    <section class="project-box">
        <div class="project-left">
            <h2>MediHelper</h2>

            <p>
                <h3>Problem Statement</h3>
                Access to reliable preliminary skin disease identification and healthcare cost estimation is limited, often leading to delayed diagnosis and poor financial planning.
            </p>
            <p>
                <h3>Solution Overview</h3>
                MediHelper leverages AI and machine learning to provide early skin disease detection from images and predict healthcare costs, empowering users with actionable health and financial insights.
            </p>
            <p>
                <h3>How I Solved the Problem</h3>
                I designed and developed MediHelper as a full-stack AI-driven web application using Django, integrating machine learning models for both medical image analysis and healthcare cost prediction. For disease detection, I trained an image classification model using TensorFlow on curated datasets of common skin conditions such as vitiligo, acne, SJS, hyperpigmentation, and nail psoriasis, enabling users to upload images and receive preliminary insights. For financial forecasting, I implemented a linear regression model that analyzes user-provided health parameters to estimate potential changes in healthcare expenses. The frontend was built with HTML and CSS to ensure a clean and informative user interface, while Pandas was used for data preprocessing and analysis. This modular architecture allows easy scalability for future features such as medicine scanning, chatbot integration, and user authentication.
            </p>

            <div class="tech-stack">
                <span>Python</span>
                <span>HTML</span>
                <span>CSS</span>
                <span>Django</span>
                <span>TensorFlow</span>
                <span>Pandas</span>
                <span>Linear Regression</span>
                <span>Image Classification (CNN-based model)g</span>
                <span>Data Preprocessing</span>
                <span>MVC Architecture</span>
                <span>OpenCV / PIL (Python Imaging Library)</span>
               

            </div>
        </div>

        <div class="project-right">
            <img src="../images/mediHelper_logo.png" alt="Project Screenshot">

            <div class="project-links">
                <a href="https://github.com/JahnviPaliwal/MediHelper">View code on GitHub</a>
                <!-- <a href="#">Working Sample</a>
                <a href="#">More details</a> -->
            </div>
        </div>
    </section>

    <!-- Add more project-box sections as needed -->

</main>

</body>
</html>








